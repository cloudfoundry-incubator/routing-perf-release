{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gorouter Throughput Performance\n",
    "\n",
    "This document summarizes results of a performance test run against Cloud Foundry's L7 HTTP router, known as Gorouter. It may also contain a comparison of two sets of results.\n",
    "\n",
    "The test results in this report can be reproduced, and this report regenerated by following instructions in the README for [Routing Performance Release](https://github.com/cloudfoundry-incubator/routing-perf-release).\n",
    "\n",
    "Click the button 'Show Code' below to toggle the display of the code used to generate the graphs below. To compare two sets of results, set `compareDatasets = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To only view one set of data, set the below variable to False and rerun all of the cells.\n",
    "compareDatasets = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Missing CPU stats file \"cpuStats.csv\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-3fab021f2297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpuStats.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Missing CPU stats file \"cpuStats.csv\"'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'perfResults.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Missing performance results file \"perfResults.csv\"'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Missing CPU stats file \"cpuStats.csv\""
     ]
    }
   ],
   "source": [
    "# SETUP: All of the imports\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = 9, 6\n",
    "matplotlib.rcParams['legend.loc'] = 'best'\n",
    "\n",
    "# We'll need these packages for plotting fit lines\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import os.path\n",
    "assert os.path.isfile('cpuStats.csv'), 'Missing CPU stats file \"cpuStats.csv\"'\n",
    "assert os.path.isfile('perfResults.csv'), 'Missing performance results file \"perfResults.csv\"'\n",
    "\n",
    "\n",
    "if compareDatasets:\n",
    "    assert os.path.isfile('old_cpuStats.csv'), 'Missing old CPU stats file \"old_cpuStats.csv\"'\n",
    "    assert os.path.isfile('old_perfResults.csv'), 'Missing old performance results file \"old_perfResults.csv\"'\n",
    "    newDataLabel = 'after'\n",
    "    oldDataLabel = 'before'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Description\n",
    "This report provides results of a long-running \"ramp up\" load test in which a fixed number of requests is sent to a static app through Gorouter from one client thread. The number of concurrent threads are then incrementally scaled and at each step the same number of requests are sent across all concurrent threads. As concurrency increases, the time required to send the same number of requests is reduced. \n",
    "\n",
    "The tests were run against a standalone Gorouter and a static app running on a raw VM, not against a full Cloud Foundry deployment. The test environment was comprised of two deployments on AWS. We elected to do our testing on Amazon Web Services (AWS) for its network stability.\n",
    "- We use a modified cf-release manifest to deploy Gorouter and NATS only. Gorouter is run on our default VM type of `c3.large`.\n",
    "- Load was generated using the [Routing Performance Release](https://github.com/cloudfoundry-incubator/routing-perf-release), which also includes the static backend and a job that registers a configurable number of routes for the backend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gorouter CPU Load over Time\n",
    "\n",
    "The following plot shows a summary of the CPU load over the duration of the test run. Here are some key points to note when looking at this data:\n",
    "\n",
    "  - Each CPU sample collected is highly variable; this is characteristic of CPU load on Gorouter when it is handling requests. For purpose of graphing, the test results have been downsampled in order to smooth out variations that happen over a short periods of time. The mean of each sample is represented in the plot to depict the average CPU load over time. As an example, while the plots may plateau before reaching 100% CPU load, actual CPU load during the sample may have fluctuated between 80% and 100% CPU load.\n",
    "  - When we test with a large routing table, as when `http_route_populator` is configured to register 100,000 routes, we see many tall spikes that show up periodically every 60 seconds. These spikes represent processing of the large number of route registration messages. When `http_route_populator` is configured to register only one route, fewer spikes are present.\n",
    "  - It's possible for the plot to be cut off before the edge of the graph. This indicates where the performance test run finished.\n",
    "  - There may be many jagged small spikes throughout the plot. This corresponds to the starting and stopping of the load tests each time concurrency is incremented.\n",
    "\n",
    "To avoid degradation of service (significant increases in latency) Gorouter should scaled vertically and/or horizontally before CPU approaches maximum. The graphs below demonstrate that as CPU approaches the plateau, latency begins to increase to the point where throughput plateaus. Past this point throughput does not increase with added load; this only results in added latency. In general we recommend scaling Gorouter when CPU reaches 60-70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To increase or decrease the number of results in the plotted sample windows, change the following variable and rerun all of the cells.\n",
    "resampleFrequency = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def processCpuData(data):\n",
    "    meanData = data.resample('{0}s'.format(resampleFrequency)).mean()\n",
    "    meanData = meanData.reset_index()\n",
    "    meanData = meanData.set_index(meanData.index.values * resampleFrequency)\n",
    "    return meanData\n",
    "\n",
    "cpuData = pd.read_csv('cpuStats.csv', parse_dates=['timestamp'], index_col=['timestamp'])\n",
    "cpuMeanData = processCpuData(cpuData)\n",
    "\n",
    "if compareDatasets:\n",
    "    oldCpuData = pd.read_csv('old_cpuStats.csv', parse_dates=['timestamp'], index_col=['timestamp'])\n",
    "    oldCpuMeanData = processCpuData(oldCpuData)\n",
    "\n",
    "Markdown(\"The following CPU data is sampled over {0} second windows.\".format(resampleFrequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = cpuMeanData.plot(ax=ax, y='percentage', c='b')\n",
    "if compareDatasets:\n",
    "    ax = oldCpuMeanData.plot(ax=ax, y='percentage', c='r')\n",
    "    ax.legend([newDataLabel, oldDataLabel])\n",
    "else:\n",
    "    ax.legend(['mean'])\n",
    "ax.set_ylabel('CPU percentage')\n",
    "ax.set_xlabel('Time since benchmark started (seconds)')\n",
    "ax.set_title('Mean CPU percentage over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gorouter Throughput over Time\n",
    "\n",
    "The following plot shows a summary of throughput performance over the duration of the test run. Here are some key points to note when looking at this data:\n",
    "\n",
    "- When we test with a large routing table, as when `http_route_populator` is configured to register 100,000 routes, we see many spikes and few troughs in this graph. These represent processing of the large number of route registration messages. When `http_route_populator` is configured to register only one route, far less variability is observed.\n",
    "- It's possible for the plot to be cut off before the edge of the graph. This indicates where the performance test run finished.\n",
    "- The throughput plot ramps up as concurrency is scaled and then plateaus. The point at which throughput plateaus should correspond with where CPU plateaus.\n",
    "\n",
    "The throughput level at which the plot plateaus is the absolute maximum throughput that each Gorouter VM will be able to handle. As can be seen in the graph below \"Headroom Plot\", by the time throughput reaches this point latency will have spiked to unacceptable levels. See comments above for \"CPU Over Time\" regarding scaling Gorouter to prevent  degradation of service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def readThroughputData(filename):\n",
    "    with open(filename) as f:\n",
    "        data = f.read()\n",
    "    # Get locations of start-time,response-time headers in file\n",
    "    header_idxs = [m.start() for m in re.finditer('start-time,response-time', data)]\n",
    "    header_idxs.append(len(data))\n",
    "    prev = header_idxs[0]\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    # Read each section delimited by the csv headers\n",
    "    for cur in header_idxs[1:]:\n",
    "        dfSection = pd.read_csv(StringIO(data[prev:cur]), parse_dates=['start-time'])\n",
    "        df = df.append(trimEdges(dfSection))\n",
    "        prev = cur\n",
    "    # Reset the index because it is a Frankenstein's monster of smaller indexes\n",
    "    df = df.reset_index().drop('index', axis=1)\n",
    "    return df\n",
    "    \n",
    "def trimEdges(data):\n",
    "    indexes = data.set_index('start-time').resample('1S').aggregate(lambda x: 1).index\n",
    "    testStartTime = indexes[1]\n",
    "    testEndTime = indexes[-2]\n",
    "    return data[(data['start-time'] >= testStartTime) & (data['start-time'] <= testEndTime)]\n",
    "    \n",
    "def processThroughputData(data):\n",
    "    buckets = data.set_index('start-time')['response-time'].resample('1S')\n",
    "    throughputData = buckets.aggregate({'throughput': lambda x: np.nan if x.count() == 0 else x.count()})\n",
    "    throughputData = throughputData.reset_index()\n",
    "    throughputData = throughputData.fillna(method='ffill')\n",
    "    return buckets, throughputData\n",
    "    \n",
    "goData = readThroughputData('perfResults.csv')\n",
    "throughputBuckets, throughputData = processThroughputData(goData)\n",
    "\n",
    "if compareDatasets:\n",
    "    oldGoData = readThroughputData('old_perfResults.csv')\n",
    "    oldThroughputBuckets, oldThroughputData = processThroughputData(oldGoData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots()\n",
    "ax = throughputData.plot(ax=ax, y='throughput', c='b')\n",
    "if compareDatasets:\n",
    "    ax = oldThroughputData.plot(ax=ax, y='throughput', c='r')\n",
    "    ax.legend([newDataLabel, oldDataLabel])\n",
    "ax.set_ylabel('Throughput (req/sec)')\n",
    "ax.set_xlabel('Time since benchmark started (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput Headroom Plot\n",
    "\n",
    "This graph plots throughput versus latency to illustrate the cost of latency as throughput approaches a limit. This graph provides Gorouter's maximum throughput to remain within a given target latency. The CPU graph above shows the CPU load at this throughput. An operator can monitor CPU, as well as latency, and scale Gorouter vertically or horizontally to remain within latency targets as application request load increases.\n",
    "In order to target throughput for application request load, divide the targeted throughput number by the throughput at the target latency and round up to the nearest whole number. This is the minimum amount of Gorouter instances (`c3.large`) necessary to meet performance objectives.\n",
    "\n",
    "For example, in order to achieve a targeted throughput of 10,000 requests per second at 20 milliseconds of average response time (latency), and if the point in the Headroom plot corresponds to 2,500 requests per second (throughput), then at least four Gorouter VMs are required to meet performance objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "goData['throughput'] = throughputBuckets.transform(len).reset_index()['response-time']\n",
    "goData.columns = ['start-time', 'latency', 'throughput']\n",
    "\n",
    "if compareDatasets:\n",
    "    oldGoData['throughput'] = oldThroughputBuckets.transform(len).reset_index()['response-time']\n",
    "    oldGoData.columns = ['start-time', 'latency', 'throughput']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateFitLine(data):\n",
    "    y, x = dmatrices('latency ~ throughput', data=data, return_type='dataframe')\n",
    "    fit = sm.GLM(y, x, family=sm.families.InverseGaussian(sm.families.links.inverse_squared)).fit()\n",
    "\n",
    "    domain = np.arange(data['throughput'].min(), data['throughput'].max())\n",
    "    predictionInputs = np.ones((len(domain), 2))\n",
    "    predictionInputs[:,1] = domain\n",
    "    fitLine = fit.predict(predictionInputs)\n",
    "    return domain, fitLine\n",
    "\n",
    "domain, goFitLine = generateFitLine(goData)\n",
    "\n",
    "if compareDatasets:\n",
    "    oldDomain, oldGoFitLine = generateFitLine(oldGoData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "if compareDatasets:\n",
    "    ax = oldGoData.plot(ax=ax, kind='scatter', x='throughput', y='latency', c='r', marker='.', alpha=0.02)\n",
    "    ax.plot(oldDomain, oldGoFitLine, c='r', lw=2) # Plot the fit line\n",
    "\n",
    "# Change the value of `c` to change the color. http://matplotlib.org/api/colors_api.html\n",
    "ax = goData.plot(ax=ax, kind='scatter', x='throughput', y='latency', c='b', marker='.', alpha=0.02)\n",
    "ax.plot(domain, goFitLine, c='b', lw=2) # Plot the fit line\n",
    "\n",
    "if compareDatasets:\n",
    "    ax.legend([oldDataLabel, newDataLabel])\n",
    "\n",
    "# To update x & y axis range change the parameters in function set_(x/y)lim(lower_limit, uppper_limit) \n",
    "ax.set_ylim(0,0.05)\n",
    "ax.set_xlim(0,3500)\n",
    "plt.xlabel('Throughput (requests/sec)')\n",
    "plt.ylabel('Latency (sec)')\n",
    "plt.title('Headroom plot', y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
